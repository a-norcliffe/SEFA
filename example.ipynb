{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6d1820",
   "metadata": {},
   "source": [
    "# Example Notebook\n",
    "\n",
    "In this notebook, we will run through a few examples to show how to use SEFA and apply it to your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a9c7b",
   "metadata": {},
   "source": [
    "Basic points:\n",
    "\n",
    "- SEFA views feature subsets using masks. The feature vector $x$ always has all features\n",
    "however, the mask vector $m \\in \\{0, 1\\}$ will tell us what is seen by the model and what is not.\n",
    "We have unit tests to make sure a feature with $0$ mask cannot affect the model output\n",
    "- At deployment we would not have a feature value, so would fill it with $0$, as long as the mask value is also $0$\n",
    "- Acquisition works with the sefa.acquire(x, mask_acq, mask_data, return_features) function.\n",
    "We use mask_acq to tell us what we have measured, and mask_data to tell us what is available to measure,\n",
    "at deployment this might be all $1$ s. The function returns the new mask, i.e. it sets one value in \n",
    "mask_acq to 1 for each instance. It does not change the value of x, so if this is used in deployment that functionality\n",
    "must be added in. There is also the ability to return the selected features as well as the mask.\n",
    "\n",
    "SEFA works with a mix of continuous and categorical features. However, to do this the continuous **must** come first in the feature vector, and\n",
    "then all categorical. Additionally, if there are categorical features SEFA must know the maximum number of categories.\n",
    "\n",
    "We give the information to SEFA when initialising in a dictionary, an example is below:\n",
    "\n",
    "sefa_config = {  \n",
    "$~~$ \"num_con_features\": 5,  \n",
    "$~~$ \"num_cat_features\": 5,  \n",
    "$~~$ \"most_categories\": 3,  \n",
    "$~~$ \"out_dim\": 10,  \n",
    "$~~$ \"latent_dim\": 4,  \n",
    "$~~$ \"num_hidden_predictor\": 2,  \n",
    "$~~$ \"hidden_dim_predictor\": 100,  \n",
    "$~~$ \"num_hidden_encoder\": 2,  \n",
    "$~~$ \"hidden_dim_encoder\": 20,  \n",
    "$~~$ \"num_samples_train\": 100,  \n",
    "$~~$ \"num_samples_predict\": 100,  \n",
    "$~~$ \"num_samples_acquire\": 100,  \n",
    "$~~$ \"beta\": 0.001,  \n",
    "$~~$ \"epochs\": 5,  \n",
    "$~~$ \"lr\": 0.001,  \n",
    "$~~$ \"batchsize\": 128,  \n",
    "$~~$ \"patience\": 5,  \n",
    "}\n",
    "\n",
    "The out_dim tells us how many classes there are in the prediction problem.\n",
    "\n",
    "To train SEFA we use a TensorDataset with X, y and M. SEFA saves the best version of\n",
    "the model during training, so the fit function requires a save path. We will be using a dummy folder and deleting it throughout\n",
    "the notebook.\n",
    "\n",
    "That is *roughly* everything you need to know, onto the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45256c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from models.sefa import SEFA\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fdd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.metrics_dict import metrics_dict\n",
    "\n",
    "auroc = metrics_dict[\"auroc\"]\n",
    "accuracy = metrics_dict[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_folder():\n",
    "  os.makedirs(\"dummy_folder\", exist_ok=True)\n",
    "\n",
    "\n",
    "def remove_dummy_folder():\n",
    "  for root, dir, files in os.walk(\"dummy_folder\", topdown=False):\n",
    "    for name in files:\n",
    "      os.remove(osp.join(root, name))\n",
    "    for name in dir:\n",
    "      os.rmdir(osp.join(root, name))\n",
    "  os.rmdir(\"dummy_folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae9283",
   "metadata": {},
   "source": [
    "# Indicator Example\n",
    "\n",
    "The first example is the indicator example from the paper. We have $d$ binary features,\n",
    "and one indicator feature taking values from $1$ to $d$. The indicator tells us which feature gives the label.\n",
    "So:\n",
    "\n",
    "$y = x_{x_{d+1}}$\n",
    "\n",
    "CMI maximization fails here because it does not select the indicator first. This is the first test, to\n",
    "make a long term acquisition. The second test is to then select the feature chosen by the indicator, which changes for each\n",
    "instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b0280",
   "metadata": {},
   "source": [
    "First we generate data as torch tensors. Even though this is categorical data, SEFA does not need one-hot encodings.\n",
    "SEFA takes the features as floats, from $0$ to $\\text{max categories} -1$. So in this case the max categories is $d$, the first $d$ features are $\\in \\{0.0, 1.0\\}$\n",
    "and the indicator is $\\in \\{0.0, ..., d-1.0\\}$.\n",
    "\n",
    "The labels are given as longs, so for this example $\\in \\{0, 1 \\}$.\n",
    "\n",
    "The masks are given as floats $\\in \\{0.0, 1.0\\}$.\n",
    "\n",
    "We put the train and val data in torch TensorDatasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69658c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_indicator_data(num_data, d):\n",
    "  xd = np.random.randint(low=0, high=2, size=(num_data, d))\n",
    "  indicator = np.random.randint(low=0, high=d, size=(num_data, 1))\n",
    "  x = np.concatenate([xd, indicator], axis=-1)\n",
    "  y = xd[np.arange(num_data), indicator.flatten()]\n",
    "  x = torch.tensor(x).float()\n",
    "  y = torch.tensor(y).long()\n",
    "  m = torch.ones_like(x)\n",
    "  return x, y, m\n",
    "\n",
    "\n",
    "d = 10\n",
    "num_train = 60_000\n",
    "num_val = 10_000\n",
    "num_test = 10_000\n",
    "\n",
    "X_train, y_train, M_train = generate_indicator_data(num_train, d)\n",
    "X_val, y_val, M_val = generate_indicator_data(num_val, d)\n",
    "X_test, y_test, M_test = generate_indicator_data(num_test, d)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train, M_train)\n",
    "val_data = TensorDataset(X_val, y_val, M_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae2582",
   "metadata": {},
   "source": [
    "Next we initialise SEFA, using the configs described previously. We have $0$ continuous features, $d+1$ categorical features, and max categories $d$.\n",
    "We have binary classification so the out_dim is $2$.\n",
    "\n",
    "All other entries are hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac0442",
   "metadata": {},
   "outputs": [],
   "source": [
    "sefa_config = {\n",
    "  \"num_con_features\": 0,\n",
    "  \"num_cat_features\": d+1,\n",
    "  \"most_categories\": d,\n",
    "  \"out_dim\": 2,\n",
    "  \"latent_dim\": 4,\n",
    "  \"num_hidden_predictor\": 2,\n",
    "  \"hidden_dim_predictor\": 100,\n",
    "  \"num_hidden_encoder\": 2,\n",
    "  \"hidden_dim_encoder\": 20,\n",
    "  \"num_samples_train\": 100,\n",
    "  \"num_samples_predict\": 100,\n",
    "  \"num_samples_acquire\": 100,\n",
    "  \"beta\": 0.001,\n",
    "  \"epochs\": 5,\n",
    "  \"lr\": 0.001,\n",
    "  \"batchsize\": 128,\n",
    "  \"patience\": 5,\n",
    "}\n",
    "\n",
    "sefa = SEFA(sefa_config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668930c7",
   "metadata": {},
   "source": [
    "Next we train SEFA with the fit function. We must give SEFA the train_data and val_data in the torch TensorDatasets.\n",
    "\n",
    "We also need to specify the save_path, where the best model is saved. This can be used in larger scale experiments if needed.\n",
    "\n",
    "Finally, we specify a predictive metric. This is used to quantify classification performance, which SEFA uses to calculate how well the acquisition has gone, and to select the best model (auroc and accuracy were functions defined at the start of the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6af2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dummy_folder()\n",
    "\n",
    "# Fit the model\n",
    "sefa.fit(train_data=train_data, val_data=val_data, save_path=\"dummy_folder\", metric_f=auroc)\n",
    "\n",
    "remove_dummy_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa22184b",
   "metadata": {},
   "source": [
    "With a trained SEFA model we can test the acquisitions. We use the .acquire function returning the selections.\n",
    "We will test if the first selection is the indicator, and if the second acquisition is the feature given by the indicator.\n",
    "\n",
    "To do this we must create a new mask_acq, full of zeros, to tell SEFA to act as if it has no features initially. We will use the .acquire function to fill mask_acq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d673a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the acquisition.\n",
    "\n",
    "X_test = X_test.to(device)\n",
    "M_test = M_test.to(device)\n",
    "m_acq = torch.zeros_like(M_test)\n",
    "\n",
    "# First acquisition\n",
    "m_acq, selected = sefa.acquire(x=X_test, mask_acq=m_acq, mask_data=M_test, return_features=True)\n",
    "print(\"Proportion of first selections that are indicator:\", (selected == d).float().mean().item())\n",
    "\n",
    "# Second selection\n",
    "m_acq, selected = sefa.acquire(x=X_test, mask_acq=m_acq, mask_data=M_test, return_features=True)\n",
    "print(\"Proportion of second selections that are given by indicator:\", (selected == X_test[:, -1]).float().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d8687",
   "metadata": {},
   "source": [
    "# Syn1\n",
    "\n",
    "Next we will look at the Syn1 example from the paper. Here we have all continuous features.\n",
    "\n",
    "The features are sampled from $\\mathcal{N}(0, 1)$.\n",
    "\n",
    "We define a logit based on value of $x_{11}$:\n",
    "\n",
    "- If $x_{11} < 0$, $l = 4x_1x_2$\n",
    "- If $x_{11} \\geq 0$, $l = 1.2 (x_3^2 + x_4^2 + x_5^2 + x_6^2) - 4.2$\n",
    "\n",
    "Then labels are sampled from $p(Y=1| x) = 1/(1+e^l)$.\n",
    "\n",
    "The optimal strategy is to select $x_{11}$ first and then the features for the relevant logit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2bd60",
   "metadata": {},
   "source": [
    "First we generate data, again $x$ and $m$ are tensors of floats, $y$ is a tensor of longs. We put the train and val data into a torch TensorDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_syn1_data(num_data):\n",
    "  x = np.random.normal(size=(num_data, 11))\n",
    "  logit1 = 4*x[:, 0]*x[:, 1]\n",
    "  logit2 = 1.2*np.sum(x[:, 2:6]**2, axis=-1) - 4.2\n",
    "  feature_11_less_0 = (x[:, -1] < 0).astype(np.float32)\n",
    "  logit = logit1*feature_11_less_0 + logit2*(1-feature_11_less_0)\n",
    "  y = np.random.binomial(1, 1/(1+np.exp(logit)), size=num_data)\n",
    "  x = torch.tensor(x).float()\n",
    "  y = torch.tensor(y).long()\n",
    "  m = torch.ones_like(x)\n",
    "  return x, y, m\n",
    "\n",
    "\n",
    "num_train = 60_000\n",
    "num_val = 10_000\n",
    "num_test = 10_000\n",
    "\n",
    "X_train, y_train, M_train = generate_syn1_data(num_train)\n",
    "X_val, y_val, M_val = generate_syn1_data(num_val)\n",
    "X_test, y_test, M_test = generate_syn1_data(num_test)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train, M_train)\n",
    "val_data = TensorDataset(X_val, y_val, M_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c21db7",
   "metadata": {},
   "source": [
    "This time in the config we have $11$ continuous features, and no categorical features or max categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sefa_config = {\n",
    "  \"num_con_features\": 11,\n",
    "  \"num_cat_features\": 0,\n",
    "  \"most_categories\": 0,\n",
    "  \"out_dim\": 2,\n",
    "  \"latent_dim\": 6,\n",
    "  \"num_hidden_predictor\": 2,\n",
    "  \"hidden_dim_predictor\": 150,\n",
    "  \"num_hidden_encoder\": 2,\n",
    "  \"hidden_dim_encoder\": 50,\n",
    "  \"num_samples_train\": 100,\n",
    "  \"num_samples_predict\": 200,\n",
    "  \"num_samples_acquire\": 200,\n",
    "  \"beta\": 0.0005,\n",
    "  \"epochs\": 30,\n",
    "  \"lr\": 0.001,\n",
    "  \"batchsize\": 128,\n",
    "  \"patience\": 5,\n",
    "}\n",
    "\n",
    "sefa = SEFA(sefa_config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63143076",
   "metadata": {},
   "source": [
    "We fit the model as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dummy_folder()\n",
    "\n",
    "# Fit the model.\n",
    "sefa.fit(train_data=train_data, val_data=val_data, save_path=\"dummy_folder\", metric_f=auroc)\n",
    "\n",
    "remove_dummy_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aac51f6",
   "metadata": {},
   "source": [
    "Next we run the acquisition, tracking which features are acquired at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run acquisition.\n",
    "\n",
    "X_test = X_test.to(device)\n",
    "M_test = M_test.to(device)\n",
    "m_acq = torch.zeros_like(M_test)\n",
    "\n",
    "selected = []\n",
    "\n",
    "for _ in range(M_test.shape[-1]):\n",
    "  m_acq, selected_tmp = sefa.acquire(x=X_test, mask_acq=m_acq, mask_data=M_test, return_features=True)\n",
    "  selected.append(selected_tmp)\n",
    "\n",
    "selected = torch.stack(selected, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b1c125",
   "metadata": {},
   "source": [
    "This code is just plotting code, to recreate the heatmaps in the paper. (First column of Figure 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5acb1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rectangle_axis(ax, left, down, right, up, linestyle=\"-\", color=\"lime\"):\n",
    "  ax.plot([left, right], [up, up], color=color, linestyle=linestyle)\n",
    "  ax.plot([left, right], [down, down], color=color, linestyle=linestyle)\n",
    "  ax.plot([left, left], [up, down], color=color, linestyle=linestyle)\n",
    "  ax.plot([right, right], [up, down], color=color, linestyle=linestyle)\n",
    "\n",
    "\n",
    "def hm_trajectories_on_axis(ax, selections):\n",
    "  # HM means heatmap, so we have a heat map behind sample trajectories.\n",
    "  selections = selections.numpy()  # Shape: [num_features, batch]\n",
    "  num_features = selections.shape[0]\n",
    "  hm = []\n",
    "  for f in range(num_features):\n",
    "    hm.append(np.mean(selections == f, axis=-1))\n",
    "  hm = np.stack(hm, axis=0)\n",
    "  ax.imshow(hm, cmap=\"Blues\", aspect=0.4, origin=\"lower\", vmin=0.0, vmax=1.0)\n",
    "\n",
    "  max_features = 6\n",
    "  trajectories = selections[:max_features, np.random.choice(a=selections.shape[1], size=400, replace=False)]\n",
    "  ax.plot(trajectories, color=\"red\", linewidth=2.5, alpha=0.018)\n",
    "  ax.set_xlim(-0.5, max_features-0.5)\n",
    "  ax.set_xticks(ticks=np.arange(max_features), labels=np.arange(max_features)+1)\n",
    "  ax.set_yticks(ticks=np.arange(num_features), labels=np.arange(num_features)+1)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, ncols=2, figsize=[8, 3.5])\n",
    "\n",
    "neg_ids = torch.where(X_test[:, -1] < 0)[0]\n",
    "pos_ids = torch.where(X_test[:, -1] >= 0.0)[0]\n",
    "\n",
    "hm_trajectories_on_axis(ax[0], selected[:, neg_ids].cpu())\n",
    "hm_trajectories_on_axis(ax[1], selected[:, pos_ids].cpu())\n",
    "\n",
    "ax[0].set_title(\"x11 < 0\")\n",
    "ax[0].set_xlabel(\"Acquisition\")\n",
    "ax[0].set_ylabel(\"Feature\")\n",
    "ax[1].set_title(\"x11 >= 0\")\n",
    "ax[1].set_xlabel(\"Acquisition\")\n",
    "ax[1].set_ylabel(\"Feature\")\n",
    "\n",
    "\n",
    "# Draw the rectangles highlighting the correct acquisitions.\n",
    "draw_rectangle_axis(ax[0], -0.5, 9.5, 0.5, 10.5)\n",
    "draw_rectangle_axis(ax[1], -0.5, 9.5, 0.5, 10.5)\n",
    "draw_rectangle_axis(ax[0], 0.5, -0.5, 2.5, 1.5)\n",
    "draw_rectangle_axis(ax[1], 0.5, 1.5, 4.5, 5.5)\n",
    "ax[0].axvline(x=2.5, color=\"black\", linestyle=\"--\")\n",
    "ax[1].axvline(x=4.5, color=\"black\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bf84a",
   "metadata": {},
   "source": [
    "# Jointly Informative Data\n",
    "\n",
    "We provide one more example of SEFA making long term acquisitions, even when there are features that provide immediate information.\n",
    "This dataset does not have instance wise orderings, but there can still be myopic behavior.\n",
    "\n",
    "The last 3 features are binary, but we translate this to be $-1$ or $1$. The label is given as the\n",
    "product of the last three features. That is, any of the three has the ability to flip the label. And it is impossible to know anything about the\n",
    "label without having all three.\n",
    "\n",
    "The other features are sampled based on the label. If $y=1$ they are sampled from a Bernoulli with\n",
    "$p=0.55$, and if $y=-1$ they are sampled from a Bernoulli with $p=0.45$. So all other features can provide some information\n",
    "about the label, even though they are noisy, therefore, CMI would select these features first.\n",
    "\n",
    "The optimal strategy is to acquire the jointly informative features first, having poor performance for three acquisitions to then have perfect performance.\n",
    "The greedy strategy that CMI would yield is to keep acquiring noisy features, slowly seeing what the most likely value of $y$ is.\n",
    "\n",
    "This example shows SEFA can acquire jointly informative (but individually uninformative) features, taking into account their possible values and interactions.\n",
    "\n",
    "(we translate $-1$ in the jointly informative features and $y$ back to $0$ after generating the noisy features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04093aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_jointly_informative_data(num_data):\n",
    "  num_ji_features = 3\n",
    "  num_noisy_features = 10\n",
    "  x_ji = 2*np.random.binomial(1, 0.5, size=(num_data, num_ji_features)) - 1\n",
    "  y = np.prod(x_ji, axis=1)\n",
    "  p = 0.5 + 0.05*y\n",
    "  p = p.reshape(-1, 1)\n",
    "  p = np.tile(p, (1, num_noisy_features))\n",
    "  x_noisy = np.random.binomial(1, p)\n",
    "  x_ji = (x_ji + 1) / 2  # Convert back to 0, 1\n",
    "  y = (y + 1) / 2\n",
    "  x = np.concatenate([x_noisy, x_ji], axis=1)  # The final three features are the jointly informative features.\n",
    "  x = torch.tensor(x).float()\n",
    "  y = torch.tensor(y).long()\n",
    "  m = torch.ones_like(x)\n",
    "  return x, y, m\n",
    "\n",
    "\n",
    "num_train = 60_000\n",
    "num_val = 10_000\n",
    "num_test = 10_000\n",
    "\n",
    "X_train, y_train, M_train = generate_jointly_informative_data(num_train)\n",
    "X_val, y_val, M_val = generate_jointly_informative_data(num_val)\n",
    "X_test, y_test, M_test = generate_jointly_informative_data(num_test)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train, M_train)\n",
    "val_data = TensorDataset(X_val, y_val, M_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b52e43",
   "metadata": {},
   "source": [
    "Again we have no continuous features, only categorical, all are binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382bd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "sefa_config = {\n",
    "  \"num_con_features\": 0,\n",
    "  \"num_cat_features\": 13,\n",
    "  \"most_categories\": 2,\n",
    "  \"out_dim\": 2,\n",
    "  \"latent_dim\": 4,\n",
    "  \"num_hidden_predictor\": 2,\n",
    "  \"hidden_dim_predictor\": 150,\n",
    "  \"num_hidden_encoder\": 2,\n",
    "  \"hidden_dim_encoder\": 50,\n",
    "  \"num_samples_train\": 100,\n",
    "  \"num_samples_predict\": 200,\n",
    "  \"num_samples_acquire\": 200,\n",
    "  \"beta\": 0.001,\n",
    "  \"epochs\": 10,\n",
    "  \"lr\": 0.001,\n",
    "  \"batchsize\": 128,\n",
    "  \"patience\": 5,\n",
    "}\n",
    "\n",
    "sefa = SEFA(sefa_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dummy_folder()\n",
    "\n",
    "# Fit the model\n",
    "sefa.fit(train_data=train_data, val_data=val_data, save_path=\"dummy_folder\", metric_f=auroc)\n",
    "\n",
    "remove_dummy_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba7a43",
   "metadata": {},
   "source": [
    "During acquisition we track the auroc. We also do this for a greedy strategy, just acquiring in order, and only acquiring the jointly informative features last.\n",
    "\n",
    "We will then plot the performance as a function of number of acquired features. And report the porportion of the first, second and third acquisitions SEFA does that are the jointly informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa95d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.to(device)\n",
    "M_test = M_test.to(device)\n",
    "m_acq = torch.zeros_like(M_test)\n",
    "m_seq = torch.zeros_like(M_test)\n",
    "\n",
    "preds_with_no_features = sefa.predict(x=X_test, mask=m_seq).detach().cpu()\n",
    "auroc_no_features = auroc(preds_with_no_features, y_test)\n",
    "metrics_acq = [auroc_no_features]\n",
    "metrics_seq = [auroc_no_features]\n",
    "\n",
    "selected = []\n",
    "\n",
    "for i in range(X_test.shape[-1]):\n",
    "  m_acq, selected_tmp = sefa.acquire(x=X_test, mask_acq=m_acq, mask_data=M_test, return_features=True)\n",
    "  m_seq[:, i] = 1.0  # Acquiring the noisy features then jointly informative features in order.\n",
    "  selected.append(selected_tmp)\n",
    "\n",
    "  preds_acq = sefa.predict(x=X_test, mask=m_acq*M_test).detach().cpu()\n",
    "  preds_seq = sefa.predict(x=X_test, mask=m_seq*M_test).detach().cpu()\n",
    "  metrics_acq.append(auroc(preds_acq, y_test))\n",
    "  metrics_seq.append(auroc(preds_seq, y_test))\n",
    "\n",
    "\n",
    "# Print proportion of selections.\n",
    "selected = torch.stack(selected, dim=0)\n",
    "print(f\"Proportion of first acquisitions being jointly informative features: {torch.mean((selected[0] > 9).float()).item()}\")\n",
    "print(f\"Proportion of second acquisitions being jointly informative features: {torch.mean((selected[1] > 9).float()).item()}\")\n",
    "print(f\"Proportion of third acquisitions being jointly informative features: {torch.mean((selected[2] > 9).float()).item()}\")\n",
    "\n",
    "\n",
    "# Plot the curves.\n",
    "metrics_acq = np.array(metrics_acq)\n",
    "metrics_seq = np.array(metrics_seq)\n",
    "plt.plot(metrics_acq, label=\"Active Acquisitions\")\n",
    "plt.plot(metrics_seq, label=\"Greedy Acquisitions\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Acquisition\")\n",
    "plt.ylabel(\"AUROC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
